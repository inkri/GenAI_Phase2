{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d0910df-f53a-4163-8821-40e1dcf42101",
   "metadata": {},
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6881ad-6f1e-42ea-b1e6-5ffd0e8e2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: Install Libraries\n",
    "#!pip install torch numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43dbfdba-c576-4c99-ac4c-b590ac87100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: Prepare Sample Dataset (Joke Text)\n",
    "# Example dataset: List of jokes\n",
    "data = [\n",
    "    \"Why did the scarecrow win an award? Because he was outstanding in his field.\",\n",
    "    \"Why donâ€™t scientists trust atoms? Because they make up everything!\",\n",
    "    \"I told my wife she was drawing her eyebrows too high. She looked surprised.\",\n",
    "    \"Parallel lines have so much in common. Itâ€™s a shame theyâ€™ll never meet.\",\n",
    "    \"What do you call fake spaghetti? An impasta.\",\n",
    "]\n",
    "\n",
    "# Combine all into one training string\n",
    "text = \"\\n\".join(data)\n",
    "\n",
    "# Build vocabulary (character-level tokenizer for simplicity)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Char-to-index and index-to-char\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# Encode / decode functions\n",
    "def encode(s): \n",
    "    return [stoi[c] if c in stoi else stoi[' '] for c in s.lower()]\n",
    "\n",
    "\n",
    "def decode(l): return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86df9913-36b2-4aa7-b564-1560bad29235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3: Create Dataset\n",
    "import torch\n",
    "\n",
    "# Encode entire corpus\n",
    "data_tensor = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Hyperparameters\n",
    "block_size = 64  # context length\n",
    "batch_size = 32\n",
    "\n",
    "# Data loader\n",
    "def get_batch():\n",
    "    ix = torch.randint(len(data_tensor) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_tensor[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_tensor[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3f060b5-7238-4553-8716-e2413938a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4: Define the Transformer Language Model\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(emb_dim, heads, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.ln1(x + attn_out)\n",
    "        ff_out = self.ff(x)\n",
    "        return self.ln2(x + ff_out)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, emb_dim)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2) * (-math.log(10000.0) / emb_dim))\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # shape: (1, max_len, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class TinyLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.positional_encoding = PositionalEncoding(emb_dim)\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(emb_dim, n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "        self.head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        x = self.positional_encoding(tok_emb)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "931a8070-45b8-4953-b2ce-2beffd67a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 3.6100\n",
      "Step 100, Loss: 0.8650\n",
      "Step 200, Loss: 0.2630\n",
      "Step 300, Loss: 0.1024\n",
      "Step 400, Loss: 0.0955\n",
      "Step 500, Loss: 0.0665\n",
      "Step 600, Loss: 0.0209\n",
      "Step 700, Loss: 0.0132\n",
      "Step 800, Loss: 0.0066\n",
      "Step 900, Loss: 0.0152\n"
     ]
    }
   ],
   "source": [
    "#STEP 5: Train the Model\n",
    "model = TinyLLM(vocab_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for step in range(1000):\n",
    "    x, y = get_batch()\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits.view(-1, vocab_size), y.view(-1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63995350-bdda-490d-8fa5-328f725fafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 6: Text Generation (Joke on demand)\n",
    "@torch.no_grad()\n",
    "def generate(prompt, max_new_tokens=100):\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor([encode(prompt)], dtype=torch.long)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    return decode(input_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e4cb294-9478-4ad5-a476-7b9f442cabc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed characters: ['\\n', ' ', '!', '.', '?', 'A', 'B', 'I', 'P', 'S', 'W', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'â€™']\n"
     ]
    }
   ],
   "source": [
    "print(\"Allowed characters:\", list(stoi.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04253d70-b5a7-48f9-ac54-3df18849745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " oke about ai dididididididididididididididididididididididididididididididididididididididididididididididiydidi\n"
     ]
    }
   ],
   "source": [
    "#Output Example\n",
    "# Try it!\n",
    "prompt = \"Joke about AI\"\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbc7d0e9-c9f3-44dc-bfab-8a14ed8ab9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " oke about scientists stsy try trytrytrytry wing!\n",
      "ingding in in istinting!\n",
      "wing ististististildnâ€™tildoldoldoldowildowhyts\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Joke about scientists\"\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97149a47-f513-4166-a905-8c90beecc5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes:\n",
    "#This is a tiny model meant for demo and learning purposes.\n",
    "#You can scale it with:\n",
    "#Bigger datasets (e.g., OpenWebText)\n",
    "#Larger model configs\n",
    "#GPU training (use .cuda() on model and tensors)\n",
    "#Token-level (e.g., BPE) instead of char-level tokenizer\n",
    "#You can also checkpoint and reload the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064141b2-f09b-4186-b619-7447ae591796",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad593306-bc19-442c-a964-9bf377090e5a",
   "metadata": {},
   "source": [
    "## 1. Load Dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0509e0-3d5c-41ba-b95b-3f98ffa2f78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000 jokes.\n",
      "{'text': '[me narrating a documentary about narrators] \"\"I can\\'t hear what they\\'re saying cuz I\\'m talking\"\"\\n'}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get a Bigger Jokes Dataset\n",
    "#We'll use the \"Fraser/shortâ€‘jokes\" dataset (~230â€¯k jokes) from Hugging Face:\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Trust remote code so Hugging Face can execute the dataset loading script\n",
    "dataset = load_dataset(\"Fraser/short-jokes\", split=\"train\", trust_remote_code=True)\n",
    "dataset = dataset.select(range(3000))\n",
    "print(f\"Loaded {len(dataset)} jokes.\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc912f70-16ad-41aa-8af4-15e090d6e084",
   "metadata": {},
   "source": [
    "## 2. Tokenize the Jokes (convert text â†’ numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5f5a136-d18b-4f35-bf60-735d492faae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105fe0b15834431b881b574180647b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step 2: Preprocess Data & Tokenize\n",
    "#Use a Byte-Pair Encoding tokenizer via Hugging Face:\n",
    "#We use GPT-2â€™s tokenizer to handle unknown tokens and subwords cleanly.\n",
    "\n",
    "#What it does:\n",
    "#Loads GPT-2â€™s Byte-Pair Encoding (BPE) tokenizer.\n",
    "#GPT-2 doesnâ€™t have a pad_token, so we reuse its eos_token (end of sentence) as padding.\n",
    "#Models can't handle text directly â€” they need numbers (token IDs).\n",
    "#This step ensures consistent sequence length (max_length=64) with padding.\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# ðŸ”¥ Fix: Set padding token to EOS token (safe for GPT-2)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006812af-fcab-47c9-9dab-9099209e9b15",
   "metadata": {},
   "source": [
    "## 3. Map Tokenizer on Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7061865-1b5e-41bf-9a50-58fa1d2d5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What it does:\n",
    "#Tokenizes each joke into input_ids and attention_mask.\n",
    "#Converts the HuggingFace dataset into PyTorch tensor format.\n",
    "#Why itâ€™s needed:\n",
    "#Tokenizing ahead of time makes training faster.\n",
    "#attention_mask tells the model which tokens to pay attention to (1 = real word, 0 = padding).\n",
    "\n",
    "# Define joke encoding function\n",
    "def encode_joke(j):\n",
    "    return tokenizer(j[\"text\"], truncation=True, max_length=64, padding=\"max_length\")\n",
    "\n",
    "# Apply encoding to the dataset\n",
    "dataset = dataset.map(encode_joke, batched=False)\n",
    "\n",
    "# Set format for PyTorch tensors\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19068b03-bd2d-46b7-a2c5-3892840541dc",
   "metadata": {},
   "source": [
    "## 4. Define Your Own Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a4db17-d346-4409-a353-93af6314bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What it does:\n",
    "#Implements a simplified GPT-style Transformer encoder.\n",
    "#Includes:\n",
    "#Token Embeddings\n",
    "#Positional Embeddings\n",
    "#Stacked Transformer layers (self-attention + feedforward)\n",
    "#LayerNorm\n",
    "#Final linear layer to project back to vocab size\n",
    "#Key Layers:\n",
    "#Layer\tPurpose\n",
    "#nn.Embedding\tConvert token IDs into vectors\n",
    "#nn.TransformerEncoderLayer\tHandles attention + context\n",
    "#LayerNorm\tNormalizes to stabilize training\n",
    "#Linear\tMaps hidden states to vocabulary for prediction\n",
    "#Why itâ€™s needed:\n",
    "#This is the core brain of your language model. It learns how words interact with each other and generates new ones.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SmallTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, n_heads=4, n_layers=4, max_len=64):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(max_len, emb_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=emb_dim,\n",
    "                nhead=n_heads,\n",
    "                dim_feedforward=emb_dim * 4,\n",
    "                activation=\"gelu\",\n",
    "                batch_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(emb_dim)\n",
    "        self.head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        b, t = input_ids.size()\n",
    "        positions = torch.arange(0, t, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.tok_emb(input_ids) + self.pos_emb(positions)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_key_padding_mask=~attention_mask.bool() if attention_mask is not None else None)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3f652-74f6-4284-b1ff-2ebc5a85bd02",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36fd9365-88f8-42d9-a247-f7744190167c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:50<00:00,  3.44s/it, loss=5.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:47<00:00,  3.35s/it, loss=3.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:45<00:00,  3.30s/it, loss=2.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#What it does:\n",
    "#Sets up the training loop:\n",
    "#Loads batches\n",
    "#Computes logits from model\n",
    "#Calculates loss vs. actual input tokens (shifted prediction)\n",
    "#Optimizes weights\n",
    "#Uses progress bar (tqdm) to track loss\n",
    "#Input:\n",
    "#Batches of shape [32, 64] â†’ 32 jokes with 64 tokens each\n",
    "\n",
    "#Why itâ€™s needed:\n",
    "#The model improves by comparing its predictions to the real next word, adjusting itself using backpropagation.\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = SmallTransformerLM(tokenizer.vocab_size).to(\"cpu\")  # CPU-based\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch in pbar:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        mask = batch[\"attention_mask\"]\n",
    "        logits = model(input_ids, attention_mask=mask)\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), input_ids.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "    print(f\"Epoch {epoch+1} done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959a188-089d-4bb2-81af-9c78ec9af2a0",
   "metadata": {},
   "source": [
    "## 6. Generate a Joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f43df38-86e1-453f-a126-40d35c28093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke about computers:::::::::::::::::::::::::::::::::::::::::::::::::::\n"
     ]
    }
   ],
   "source": [
    "#What it does:\n",
    "#Takes a prompt like \"Joke about computers:\"\n",
    "#Uses the model to predict next tokens one-by-one\n",
    "#Applies:\n",
    "#Temperature scaling (controls randomness)\n",
    "#Top-k sampling (keeps only most likely 50 words)\n",
    "#Multinomial sampling (adds variation)\n",
    "#Why itâ€™s needed:\n",
    "#Sampling lets you use the trained model to generate new creative jokes, not just repeat what it saw in training.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_joke(prompt, max_new=50, temperature=0.8, top_k=50):\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    for _ in range(max_new):\n",
    "        logits = model(tokens)\n",
    "        next_logits = logits[:, -1, :] / temperature  # shape: [1, vocab_size]\n",
    "        \n",
    "        # Top-k filtering\n",
    "        topk = torch.topk(next_logits, top_k)\n",
    "        topk_logits = topk.values\n",
    "        topk_indices = topk.indices\n",
    "\n",
    "        # Softmax over filtered logits\n",
    "        probs = F.softmax(topk_logits, dim=-1)  # shape: [1, top_k]\n",
    "\n",
    "        # Sample from top-k probs\n",
    "        next_token_id = topk_indices[0, torch.multinomial(probs[0], 1)]\n",
    "\n",
    "        # Append new token to sequence\n",
    "        tokens = torch.cat([tokens, next_token_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "    return tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Try generating!\n",
    "print(generate_joke(\"Joke about computers:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7814d3df-7dd4-4d85-905c-a0008aff7bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke about cats:::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "Joke about doctors:::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "Joke about AI:::::::::::::::::::::::::::::::::::::::::::::::::::\n"
     ]
    }
   ],
   "source": [
    "print(generate_joke(\"Joke about cats:\"))\n",
    "print(generate_joke(\"Joke about doctors:\"))\n",
    "print(generate_joke(\"Joke about AI:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1a2ec-aaaf-424c-b74a-cd46f1c81de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae154239-ddbf-4d6b-9439-58d3e4f3084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapping Up\n",
    "#Larger dataset (~230k jokes) improves model generalization \n",
    "#Tokenizer handles unknown text properly.\n",
    "#Transformer-based encoder supports rich attention patterns.\n",
    "#Sampling produces diverse and creative outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "458a81c8-319b-478b-b0aa-64d2f651fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tips to Refine Further\n",
    "#Switch to decoder-only transformer for true autoregressive behavior.\n",
    "#Train longer (more epochs).\n",
    "#Add validation split and track perplexity.\n",
    "#Export weights and build a Gradio interface for easy use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae2268-2e69-427d-a956-1d608008a59c",
   "metadata": {},
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84583f-448d-4cc8-8d5f-4ea714374237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 30000 jokes.\n",
      "ðŸ§¾ Example joke: [me narrating a documentary about narrators] \"\"I can't hear what they're saying cuz I'm talking\"\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354122b8c9a2447aab6821e04f2c0bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [50:49<00:00,  1.63s/it, loss=0.481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 1 completed. Avg Loss: 1.8649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [51:09<00:00,  1.64s/it, loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 2 completed. Avg Loss: 0.2247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [50:19<00:00,  1.61s/it, loss=0.0914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 3 completed. Avg Loss: 0.0616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [3:19:35<00:00,  6.39s/it, loss=0.00465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 4 completed. Avg Loss: 0.0102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [1:05:25<00:00,  2.09s/it, loss=0.0017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 5 completed. Avg Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [1:50:09<00:00,  3.53s/it, loss=0.00362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 6 completed. Avg Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 1148/1875 [1:55:51<19:07,  1.58s/it, loss=0.00195]"
     ]
    }
   ],
   "source": [
    "# Step 1: Load a small dataset (limit to 3000 jokes)\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Fraser/short-jokes\", split=\"train\", trust_remote_code=True)\n",
    "dataset = dataset.select(range(30000))  # Limit for faster CPU training\n",
    "print(f\"âœ… Loaded {len(dataset)} jokes.\")\n",
    "print(\"ðŸ§¾ Example joke:\", dataset[0][\"text\"])\n",
    "\n",
    "# Step 2: Load GPT-2 tokenizer and fix padding\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have pad_token, use eos\n",
    "\n",
    "# Step 3: Tokenize the dataset\n",
    "def encode_joke(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=64, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(encode_joke, batched=False)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# Step 4: Define small GPT-style Transformer model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SmallTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, n_heads=4, n_layers=4, max_len=64):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding = nn.Embedding(max_len, emb_dim)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=emb_dim,\n",
    "                nhead=n_heads,\n",
    "                dim_feedforward=emb_dim * 4,\n",
    "                activation=\"gelu\",\n",
    "                batch_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(emb_dim)\n",
    "        self.output_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        B, T = input_ids.size()\n",
    "        pos_ids = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(pos_ids)\n",
    "\n",
    "        for layer in self.transformer_blocks:\n",
    "            x = layer(x, src_key_padding_mask=~attention_mask.bool() if attention_mask is not None else None)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return self.output_head(x)\n",
    "\n",
    "# Step 5: Training loop\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = SmallTransformerLM(tokenizer.vocab_size).to(\"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(10):  # Try more for better results\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch in pbar:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        mask = batch[\"attention_mask\"]\n",
    "\n",
    "        # Shift input and label for next-token prediction\n",
    "        input_ids_in = input_ids[:, :-1]\n",
    "        labels = input_ids[:, 1:]\n",
    "        mask_in = mask[:, :-1]\n",
    "\n",
    "        logits = model(input_ids_in, attention_mask=mask_in)\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"âœ… Epoch {epoch+1} completed. Avg Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "# Step 6: Generate jokes\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_joke(prompt, max_new=50, temperature=0.8, top_k=50):\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    prompt_len = tokens.shape[1]\n",
    "\n",
    "    for _ in range(max_new):\n",
    "        logits = model(tokens)\n",
    "        next_logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        topk = torch.topk(next_logits, top_k)\n",
    "        topk_logits = topk.values\n",
    "        topk_indices = topk.indices\n",
    "\n",
    "        probs = F.softmax(topk_logits, dim=-1)\n",
    "        sampled_idx = torch.multinomial(probs[0], 1).item()\n",
    "        next_token_id = topk_indices[0, sampled_idx]\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        tokens = torch.cat([tokens, next_token_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "\n",
    "    return tokenizer.decode(tokens[0][prompt_len:], skip_special_tokens=True)\n",
    "\n",
    "# Step 7: Try generating some jokes\n",
    "print(\"\\nðŸ§ª Generated Jokes:\")\n",
    "print(\"ðŸ± Joke about cats:\", generate_joke(\"Joke about cats:\"))\n",
    "print(\"ðŸ¤– Joke about AI:\", generate_joke(\"Joke about AI:\"))\n",
    "print(\"ðŸ©º Joke about doctors:\", generate_joke(\"Joke about doctors:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d78d5a-567a-4d4c-8fd1-612bce8c6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_joke(\"Joke a funny one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5246bb-7065-4438-8f0d-30aad0608670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add This After Training Loop (to Save Model)\n",
    "# ðŸ”½ Step 5.5: Save the model and tokenizer\n",
    "import os\n",
    "\n",
    "save_dir = \"small_transformer_gpt\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model state dict\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, \"model.pt\"))\n",
    "\n",
    "# Save tokenizer (config + vocab files)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"âœ… Model and tokenizer saved to '{save_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18afc9-bbd7-4f45-a47b-7cd0ac18d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Load Later (in a new notebook or script)\n",
    "# ðŸ”¼ Step 0: Load model and tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"small_transformer_gpt\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define model class again (must match saved model)\n",
    "class SmallTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, n_heads=4, n_layers=4, max_len=64):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding = nn.Embedding(max_len, emb_dim)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=emb_dim,\n",
    "                nhead=n_heads,\n",
    "                dim_feedforward=emb_dim * 4,\n",
    "                activation=\"gelu\",\n",
    "                batch_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(emb_dim)\n",
    "        self.output_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        B, T = input_ids.size()\n",
    "        pos_ids = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(pos_ids)\n",
    "        for layer in self.transformer_blocks:\n",
    "            x = layer(x, src_key_padding_mask=~attention_mask.bool() if attention_mask is not None else None)\n",
    "        x = self.ln_f(x)\n",
    "        return self.output_head(x)\n",
    "\n",
    "# Reinitialize and load weights\n",
    "model = SmallTransformerLM(tokenizer.vocab_size).to(\"cpu\")\n",
    "model.load_state_dict(torch.load(\"small_transformer_gpt/model.pt\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model loaded and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e85191-e869-425e-91a5-7c7e029390f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then You Can Use Your generate_joke() Function As-Is\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_joke(prompt, max_new=50, temperature=0.8, top_k=50):\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    prompt_len = tokens.shape[1]\n",
    "\n",
    "    for _ in range(max_new):\n",
    "        logits = model(tokens)\n",
    "        next_logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        topk = torch.topk(next_logits, top_k)\n",
    "        topk_logits = topk.values\n",
    "        topk_indices = topk.indices\n",
    "\n",
    "        probs = F.softmax(topk_logits, dim=-1)\n",
    "        sampled_idx = torch.multinomial(probs[0], 1).item()\n",
    "        next_token_id = topk_indices[0, sampled_idx]\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        tokens = torch.cat([tokens, next_token_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "\n",
    "    return tokenizer.decode(tokens[0][prompt_len:], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "print(generate_joke(\"Joke about phones:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85eb20f-296c-4d29-b155-e5a962bd630d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589af49c-8f03-4e62-87c3-e62c560c4e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028738e1-7613-42c0-8019-6249f2e46e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e5dd4-317a-404e-91f5-f3e9491dadb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a2a33-982d-4c39-a6ec-08b87bbe3bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d9ed0e-cabb-484a-9f3f-65856209c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of Fixes & Improvements:\n",
    "#âœ… Area\tâœ… Fix\n",
    "#Padding issue\tExplicitly set tokenizer.pad_token = tokenizer.eos_token\n",
    "#Multinomial crash\tProperly index into top-k logits\n",
    "#Loop behavior\tAdded EOS token stop condition\n",
    "#Batch size\tReduced to 16 for CPU memory efficiency\n",
    "#Training duration\tIncreased epochs from 3 to 5\n",
    "#Model capacity\tKept small for CPU feasibility\n",
    "#Tokenizer decode\tskip_special_tokens=True to clean output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dee12e-8b63-4e78-a2b8-592ebcd42291",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0]['input_ids'][:10])  # Token IDs\n",
    "print(tokenizer.decode(dataset[0]['input_ids']))  # Joke string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d5910b-5db9-4fb6-874e-30c37d3a1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Save the Model and Tokenizer\n",
    "import os\n",
    "\n",
    "# Directory to save model\n",
    "save_dir = \"my_joke_model\"\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, \"model.pt\"))\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"âœ… Model and tokenizer saved to '{save_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bea57f-8e47-434f-9f6a-9f81c56f3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Load the Model and Tokenizer Later\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "# === Match architecture used before ===\n",
    "class SmallTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, n_heads=4, n_layers=4, max_len=64):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding = nn.Embedding(max_len, emb_dim)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=emb_dim,\n",
    "                nhead=n_heads,\n",
    "                dim_feedforward=emb_dim * 4,\n",
    "                activation=\"gelu\",\n",
    "                batch_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(emb_dim)\n",
    "        self.output_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        B, T = input_ids.size()\n",
    "        pos_ids = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(pos_ids)\n",
    "\n",
    "        for layer in self.transformer_blocks:\n",
    "            x = layer(x, src_key_padding_mask=~attention_mask.bool() if attention_mask is not None else None)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return self.output_head(x)\n",
    "\n",
    "# === Load ===\n",
    "load_dir = \"my_joke_model\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Rebuild model and load weights\n",
    "model = SmallTransformerLM(tokenizer.vocab_size)\n",
    "model.load_state_dict(torch.load(os.path.join(load_dir, \"model.pt\"), map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model and tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de15bc-32e5-4388-9737-c6101fdb4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Test Joke Generation Again\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_joke(prompt, max_new=50, temperature=0.8, top_k=50):\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    for _ in range(max_new):\n",
    "        logits = model(tokens)\n",
    "        next_logits = logits[:, -1, :] / temperature\n",
    "        topk = torch.topk(next_logits, top_k)\n",
    "        topk_logits = topk.values\n",
    "        topk_indices = topk.indices\n",
    "\n",
    "        probs = F.softmax(topk_logits, dim=-1)\n",
    "        sampled_idx = torch.multinomial(probs[0], 1).item()\n",
    "        next_token_id = topk_indices[0, sampled_idx]\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        tokens = torch.cat([tokens, next_token_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "    return tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate_joke(\"Joke about cats:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ccae2-2f4c-493b-8928-f94f978aaa68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd85ff-6342-4d0a-b4b9-38866192e0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7358be-02c8-45db-a59f-9fe40f0e9d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
